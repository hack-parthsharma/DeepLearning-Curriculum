# Chapter 9: Adversarial Training

Adversarial training is designed to make models robust to adversarially-selected inputs.

## Recommended reading

- [Adversarial examples](https://arxiv.org/abs/1312.6199) - A famous result showing that image classifiers are vulnerable to adversarial attacks, in which an image can be imperceptibly perturbed to cause the classifier to select an incorrect target class with high confidence. Training on adversarial examples makes more models robust to them, and is an example of adversarial training.

## Optional reading

- [Adversarial Examples Are Not Bugs, They Are Features](https://gradientscience.org/adv/) - A proposed explanation for adversarial examples, that they are the result of so-called "non-robust" features. Further discussion of these results can be found [here](https://distill.pub/2019/advex-bugs-discussion/).
- [Transfer of Adversarial Robustness Between Perturbation Types](https://arxiv.org/abs/1905.01034) - There are different ways of bounding adversarial perturbations so that they remain imperceptible, and this paper studies how well adversarial training transfers between them.
- [Red Teaming Language Models with Language Models](https://arxiv.org/abs/2202.03286) - In the context of language models, an analogous method can be used to make models robust to adversarial inputs designed to cause the model to output harmful or unwanted content.
- [Adversarial Training for High-Stakes Reliability](https://arxiv.org/abs/2205.01663) - A study of adversarial training for language models using a sequence of increasingly powerful adversaries. Further discussion motivating this work can be found [here](https://www.alignmentforum.org/posts/A9tJFJY7DsGTFKKkh/high-stakes-alignment-via-adversarial-training-redwood). One possible extension that could help with inner alignment is relaxed adversarial training, in which the adversary's task is "relaxed" to allow ["pseudo-inputs"](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d#c423).

## Suggested exercise

Red team a GPT-2 chatbot to find inputs where it generates offensive language, reproducing the experimental setup in the [red teaming paper](https://arxiv.org/abs/2202.03286). We recommend using all models via [HuggingFace Transformers](https://github.com/huggingface/transformers) in an environment with GPUs available ([Google Colab](https://colab.research.google.com/) provides GPUs for free).

- Choose a language model (LM) for red teaming. We recommend GPT-2 (or larger) as the LM.
- Choose a chatbot-like model to red team. We recommend using the prompt from the last page of the red teaming paper to prompt GPT-2 (or larger) to generate chatbot-like text.
- Use an offensive or toxic language detection model of your choice. We recommend [Unitaryâ€™s BERT-based model](https://huggingface.co/unitary/toxic-bert) or a similar toxicity classifier available on HuggingFace.
- Use the zero-shot approach described in the red teaming paper (section 3.1) to generate inputs that elicit offensive language from the chatbot language model. Look for patterns in the failed test cases, to better understand what kinds of inputs the chatbot fails on.
- (Optional) Use the few-shot, supervised learning, and reinforcement learning approaches (in that order) to generate even harder test cases for the chatbot. How do the test cases generated by each method differ from each other?
- (Optional) Reproduce various analyses in the red teaming paper, e.g., clustering the test cases, to help find common patterns in the chatbot failures.

Credit to Ethan Perez for this suggested exercise.
